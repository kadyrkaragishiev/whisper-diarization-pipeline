#!/usr/bin/env python3
"""
Whisper + PyAnnote Audio Pipeline
–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤
"""

import os
import sys
import warnings
import tempfile
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import click
import torch
import whisper
import librosa
import soundfile as sf
from pyannote.audio import Pipeline
from pyannote.core import Segment
import pandas as pd
from tqdm import tqdm
import time

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings("ignore")


class AudioProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è"""
    
    def __init__(self, whisper_model: str = "base", hf_token: Optional[str] = None, 
                 local_models_dir: Optional[str] = None, device: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            whisper_model: –ú–æ–¥–µ–ª—å Whisper (tiny, base, small, medium, large)
            hf_token: HuggingFace —Ç–æ–∫–µ–Ω –¥–ª—è PyAnnotate
            local_models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (cpu, cuda, mps)
        """
        self.whisper_model_name = whisper_model
        self.hf_token = hf_token
        self.local_models_dir = Path(local_models_dir) if local_models_dir else None
        
        # –í—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if device is not None:
            self.device = device
        else:
            if torch.cuda.is_available():
                self.device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = "mps"  # Apple Silicon
            else:
                self.device = "cpu"
            
        print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
        self._load_models()
    
    def _load_local_config(self) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π"""
        if not self.local_models_dir:
            return None
            
        config_file = self.local_models_dir / "local_config.json"
        if not config_file.exists():
            return None
            
        try:
            with open(config_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return None
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π Whisper –∏ PyAnnotate"""
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Whisper...")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å Whisper
        whisper_device = self.device
        try:
            self.whisper_model = whisper.load_model(
                self.whisper_model_name, 
                device=whisper_device
            )
            print(f"‚úÖ –ú–æ–¥–µ–ª—å Whisper –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {whisper_device}")
        except Exception as e:
            if whisper_device == "mps" and ("SparseMPS" in str(e) or "aten::empty.memory_format" in str(e) or "_sparse_coo_tensor_with_dims_and_tensors" in str(e)):
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Whisper –Ω–∞ MPS: {e}")
                print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU –¥–ª—è Whisper...")
                whisper_device = "cpu"
                self.whisper_model = whisper.load_model(
                    self.whisper_model_name, 
                    device=whisper_device
                )
                print("‚úÖ –ú–æ–¥–µ–ª—å Whisper –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")
            else:
                raise e
        
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏...")
        self.diarization_pipeline = None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        local_config = self._load_local_config()
        
        if local_config and self.local_models_dir:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            try:
                pyannote_model_path = self.local_models_dir / "pyannote" / "speaker-diarization-3.1"
                
                if pyannote_model_path.exists():
                    print(f"üè† –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–∑: {pyannote_model_path}")
                    self.diarization_pipeline = Pipeline.from_pretrained(str(pyannote_model_path))
                    
                    if self.device != "cpu":
                        self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
                    
                    print("‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                    return
                else:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫–µ—à–∞ –±–µ–∑ —Ç–æ–∫–µ–Ω–∞
                    print("üîÑ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –∫–µ—à–∞...")
                    self.diarization_pipeline = Pipeline.from_pretrained(
                        "pyannote/speaker-diarization-3.1"
                    )
                    
                    if self.device != "cpu":
                        self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
                    
                    print("‚úÖ –ú–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫–µ—à–∞")
                    return
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
                print("üîÑ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ HuggingFace...")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫–µ—à–∞ –±–µ–∑ —Ç–æ–∫–µ–Ω–∞ (–µ—Å–ª–∏ –Ω–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)
        try:
            print("üîÑ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –∫–µ—à–∞...")
            self.diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1"
            )
            
            if self.device != "cpu":
                self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
            
            print("‚úÖ –ú–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫–µ—à–∞")
            return
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–µ—à–∞: {e}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ HuggingFace
        try:
            if not self.hf_token:
                print("‚ö†Ô∏è  HuggingFace —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")
                print("üí° –î–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏")
                print("üí° –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏: python download_models.py")
                return
                
            self.diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=self.hf_token
            )
            
            if self.device != "cpu":
                self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
                
            print("‚úÖ –ú–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ HuggingFace")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å HuggingFace —Ç–æ–∫–µ–Ω –∏ –¥–æ—Å—Ç—É–ø –∫ pyannote/speaker-diarization-3.1")
            print("üí° –ò–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ: python download_models.py")
            self.diarization_pipeline = None
    
    def _prepare_audio(self, audio_path: str) -> str:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            
        Returns:
            –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
        """
        audio_path = Path(audio_path)
        
        if not audio_path.exists():
            raise FileNotFoundError(f"–ê—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")
        
        # –ï—Å–ª–∏ —É–∂–µ wav —Ñ–∞–π–ª, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if audio_path.suffix.lower() == '.wav':
            return str(audio_path)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ wav
        print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –≤ WAV —Ñ–æ—Ä–º–∞—Ç...")
        audio, sr = librosa.load(str(audio_path), sr=16000)
        
        temp_wav = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
        sf.write(temp_wav.name, audio, sr)
        
        return temp_wav.name
    
    def transcribe(self, audio_path: str) -> Dict:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é Whisper
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        """
        print("üé§ –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é...")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–∞ –Ω–∞—á–∞–ª–∞ –∞—É–¥–∏–æ
        transcribe_options = {
            "language": "ru",
            "word_timestamps": True,
            "initial_prompt": "",  # –ü—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–∞
            "temperature": 0.0,    # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            "no_speech_threshold": 0.4,  # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–µ—á–∏
            "logprob_threshold": -1.0,   # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            "compression_ratio_threshold": 2.4,  # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–∞
        }
        
        # –î–ª—è –º–æ–¥–µ–ª–µ–π small –∏ medium –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        if self.whisper_model_name in ['small', 'medium']:
            print("üîß –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è small/medium –º–æ–¥–µ–ª–∏...")
            transcribe_options.update({
                "temperature": 0.1,    # –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É
                "no_speech_threshold": 0.3,  # –ï—â–µ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥
                "condition_on_previous_text": False,  # –û—Ç–∫–ª—é—á–∞–µ–º —É—Å–ª–æ–≤–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            })
        
        result = self.whisper_model.transcribe(
            audio_path,
            **transcribe_options
        )
        
        return result
    
    def diarize(self, audio_path: str, min_speakers: int = 1, max_speakers: int = 10, 
                min_segment_duration: float = 0.5) -> Optional[Dict]:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            min_speakers: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤
            max_speakers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤
            min_segment_duration: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞ (—Å–µ–∫)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ None –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
        """
        if self.diarization_pipeline is None:
            print("‚ö†Ô∏è  –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ - –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return None
            
        print("üë• –ù–∞—á–∏–Ω–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å–ø–∏–∫–µ—Ä–æ–≤...")
        print(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: —Å–ø–∏–∫–µ—Ä—ã {min_speakers}-{max_speakers}, –º–∏–Ω. —Å–µ–≥–º–µ–Ω—Ç {min_segment_duration}—Å")
        
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            diarization_params = {
                "min_speakers": min_speakers,
                "max_speakers": max_speakers
            }
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            print("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞—É–¥–∏–æ...")
            diarization = self.diarization_pipeline(audio_path, **diarization_params)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            speakers = []
            segment_count_before = 0
            
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                segment_count_before += 1
                duration = turn.end - turn.start
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã
                if duration >= min_segment_duration:
                    speakers.append({
                        "start": turn.start,
                        "end": turn.end,
                        "speaker": speaker,
                        "duration": duration
                    })
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            speakers = self._merge_consecutive_same_speaker(speakers)
            
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤ –≤ –ø–æ–Ω—è—Ç–Ω—ã–µ –∏–º–µ–Ω–∞
            speakers = self._rename_speakers(speakers)
            
            print(f"üìä –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {segment_count_before} ‚Üí {len(speakers)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            print(f"üë• –ù–∞–π–¥–µ–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(set(s['speaker'] for s in speakers))}")
            
            return {
                "speakers": speakers,
                "stats": {
                    "segments_before_filter": segment_count_before,
                    "segments_after_filter": len(speakers),
                    "unique_speakers": len(set(s['speaker'] for s in speakers))
                }
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return None
    
    def _merge_consecutive_same_speaker(self, speakers: List[Dict], gap_threshold: float = 0.3) -> List[Dict]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø–∞—É–∑–∞–º–∏
        
        Args:
            speakers: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å–ø–∏–∫–µ—Ä–æ–≤
            gap_threshold: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è (—Å–µ–∫)
        """
        if not speakers:
            return speakers
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞
        speakers = sorted(speakers, key=lambda x: x['start'])
        merged = [speakers[0]]
        
        for current in speakers[1:]:
            last = merged[-1]
            
            # –ï—Å–ª–∏ —Ç–æ—Ç –∂–µ —Å–ø–∏–∫–µ—Ä –∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ –Ω–µ–±–æ–ª—å—à–æ–π - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
            if (current['speaker'] == last['speaker'] and 
                current['start'] - last['end'] <= gap_threshold):
                
                # –†–∞—Å—à–∏—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
                merged[-1] = {
                    "start": last['start'],
                    "end": current['end'],
                    "speaker": last['speaker'],
                    "duration": current['end'] - last['start']
                }
            else:
                merged.append(current)
        
        return merged
    
    def _rename_speakers(self, speakers: List[Dict]) -> List[Dict]:
        """
        –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ—Ç —Å–ø–∏–∫–µ—Ä–æ–≤ –≤ –ø–æ–Ω—è—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ (–°–ø–∏–∫–µ—Ä 1, –°–ø–∏–∫–µ—Ä 2, etc.)
        """
        if not speakers:
            return speakers
        
        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ –ø–æ—è–≤–ª–µ–Ω–∏—è
        unique_speakers = []
        for speaker_data in speakers:
            speaker = speaker_data['speaker']
            if speaker not in unique_speakers:
                unique_speakers.append(speaker)
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ —Å—Ç–∞—Ä—ã—Ö –∏–º–µ–Ω –Ω–∞ –Ω–æ–≤—ã–µ
        speaker_mapping = {}
        for i, old_name in enumerate(unique_speakers):
            speaker_mapping[old_name] = f"–°–ø–∏–∫–µ—Ä {i + 1}"
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ–≤—ã–µ –∏–º–µ–Ω–∞
        for speaker_data in speakers:
            speaker_data['speaker'] = speaker_mapping[speaker_data['speaker']]
        
        return speakers
    
    def _align_transcription_with_speakers(self, transcription: Dict, diarization: Dict, 
                                          alignment_strategy: str = "smart") -> List[Dict]:
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–æ–≤–º–µ—â–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–ø–∏–∫–µ—Ä–∞—Ö
        
        Args:
            transcription: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            diarization: –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            alignment_strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–≤–º–µ—â–µ–Ω–∏—è ('strict', 'smart', 'aggressive')
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ —Å–ø–∏–∫–µ—Ä–∞–º–∏
        """
        if not diarization or "speakers" not in diarization:
            # –ï—Å–ª–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
            segments = []
            for segment in transcription.get("segments", []):
                segments.append({
                    "start": segment["start"],
                    "end": segment["end"],
                    "text": segment["text"].strip(),
                    "speaker": "Unknown"
                })
            return segments
        
        # –°–æ–≤–º–µ—â–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é
        aligned_segments = []
        speaker_segments = diarization["speakers"]
        transcription_segments = transcription.get("segments", [])
        
        print(f"üîÑ –°–æ–≤–º–µ—â–µ–Ω–∏–µ —Å —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π '{alignment_strategy}'...")
        
        for t_segment in transcription_segments:
            t_start, t_end = t_segment["start"], t_segment["end"]
            t_mid = (t_start + t_end) / 2
            
            best_speaker = self._find_best_speaker(
                t_start, t_end, t_mid, speaker_segments, alignment_strategy
            )
            
            aligned_segments.append({
                "start": t_start,
                "end": t_end,
                "text": t_segment["text"].strip(),
                "speaker": best_speaker
            })
        
        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: —É—Å—Ç—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è Unknown —Å–µ–≥–º–µ–Ω—Ç—ã
        aligned_segments = self._resolve_unknown_speakers(aligned_segments, speaker_segments)
        
        return aligned_segments
    
    def _find_best_speaker(self, t_start: float, t_end: float, t_mid: float, 
                          speaker_segments: List[Dict], strategy: str) -> str:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        """
        best_speaker = "Unknown"
        best_score = 0
        
        for s_segment in speaker_segments:
            s_start, s_end = s_segment["start"], s_segment["end"]
            score = 0
            
            if strategy == "strict":
                # –¢–æ–ª—å–∫–æ —Å—Ç—Ä–æ–≥–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
                overlap_start = max(t_start, s_start)
                overlap_end = min(t_end, s_end)
                overlap = max(0, overlap_end - overlap_start)
                score = overlap
                
            elif strategy == "smart":
                # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
                # 1. –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
                overlap_start = max(t_start, s_start)
                overlap_end = min(t_end, s_end)
                overlap = max(0, overlap_end - overlap_start)
                
                if overlap > 0:
                    score = overlap * 10  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è–º
                else:
                    # 2. –ë–ª–∏–∑–æ—Å—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                    gap_to_start = abs(t_mid - s_start)
                    gap_to_end = abs(t_mid - s_end)
                    min_gap = min(gap_to_start, gap_to_end)
                    
                    # 3. –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
                    s_mid = (s_start + s_end) / 2
                    gap_to_mid = abs(t_mid - s_mid)
                    
                    # –ß–µ–º –±–ª–∏–∂–µ, —Ç–µ–º –≤—ã—à–µ —Å—á–µ—Ç (–Ω–æ –º–µ–Ω—å—à–µ —á–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ)
                    if min_gap < 2.0:  # –ú–∞–∫—Å–∏–º—É–º 2 —Å–µ–∫—É–Ω–¥—ã —Ä–∞–∑—Ä—ã–≤–∞
                        score = max(0, 2.0 - min_gap)
                    elif gap_to_mid < 3.0:  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –ø–æ —Å—Ä–µ–¥–Ω–µ–π —Ç–æ—á–∫–µ
                        score = max(0, 1.0 - gap_to_mid / 3.0)
                        
            elif strategy == "aggressive":
                # –û—á–µ–Ω—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–æ–≤–º–µ—â–µ–Ω–∏–µ
                # –õ—é–±–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∏–ª–∏ –±–ª–∏–∑–æ—Å—Ç—å
                overlap_start = max(t_start, s_start)
                overlap_end = min(t_end, s_end)
                overlap = max(0, overlap_end - overlap_start)
                
                if overlap > 0:
                    score = overlap * 10
                else:
                    # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–µ–π —Ç–æ—á–∫–∏
                    distances = [
                        abs(t_start - s_start), abs(t_start - s_end),
                        abs(t_end - s_start), abs(t_end - s_end),
                        abs(t_mid - s_start), abs(t_mid - s_end)
                    ]
                    min_distance = min(distances)
                    
                    if min_distance < 5.0:  # –î–æ 5 —Å–µ–∫—É–Ω–¥ —Ä–∞–∑—Ä—ã–≤–∞
                        score = max(0, 5.0 - min_distance)
            
            if score > best_score:
                best_score = score
                best_speaker = s_segment["speaker"]
        
        return best_speaker
    
    def _resolve_unknown_speakers(self, segments: List[Dict], speaker_segments: List[Dict]) -> List[Dict]:
        """
        –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è Unknown —Å–ø–∏–∫–µ—Ä–æ–≤
        """
        if not speaker_segments:
            return segments
            
        # –°–æ–±–∏—Ä–∞–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤
        known_speakers = [s["speaker"] for s in speaker_segments]
        unknown_segments = [i for i, seg in enumerate(segments) if seg["speaker"] == "Unknown"]
        
        if not unknown_segments:
            return segments
            
        print(f"üîß –ò—Å–ø—Ä–∞–≤–ª—è–µ–º {len(unknown_segments)} Unknown —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
        
        for idx in unknown_segments:
            segment = segments[idx]
            t_start, t_end = segment["start"], segment["end"]
            t_mid = (t_start + t_end) / 2
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ë–ª–∏–∂–∞–π—à–∏–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ø–∏–∫–µ—Ä –∏–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            best_speaker = self._find_nearest_speaker(t_mid, speaker_segments)
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            if best_speaker == "Unknown":
                best_speaker = self._find_contextual_speaker(idx, segments, known_speakers)
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –°–∞–º—ã–π —á–∞—Å—Ç—ã–π —Å–ø–∏–∫–µ—Ä –≤ –∞—É–¥–∏–æ
            if best_speaker == "Unknown" and known_speakers:
                speaker_counts = {}
                for s in segments:
                    if s["speaker"] != "Unknown":
                        speaker_counts[s["speaker"]] = speaker_counts.get(s["speaker"], 0) + 1
                
                if speaker_counts:
                    best_speaker = max(speaker_counts, key=speaker_counts.get)
                else:
                    best_speaker = known_speakers[0] if known_speakers else "–°–ø–∏–∫–µ—Ä 1"
            
            segments[idx]["speaker"] = best_speaker
        
        return segments
    
    def _find_nearest_speaker(self, t_mid: float, speaker_segments: List[Dict]) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç –±–ª–∏–∂–∞–π—à–µ–≥–æ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–ø–∏–∫–µ—Ä–∞"""
        min_distance = float('inf')
        nearest_speaker = "Unknown"
        
        for s_segment in speaker_segments:
            s_start, s_end = s_segment["start"], s_segment["end"]
            s_mid = (s_start + s_end) / 2
            
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Å—Ä–µ–¥–Ω–µ–π —Ç–æ—á–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            distance = abs(t_mid - s_mid)
            
            if distance < min_distance:
                min_distance = distance
                nearest_speaker = s_segment["speaker"]
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ —Ä–∞–∑—É–º–Ω–æ–µ (< 10 —Å–µ–∫—É–Ω–¥)
        return nearest_speaker if min_distance < 10.0 else "Unknown"
    
    def _find_contextual_speaker(self, idx: int, segments: List[Dict], known_speakers: List[str]) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–ø–∏–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
        # –°–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        before_speaker = None
        after_speaker = None
        
        # –ü—Ä–µ–¥—ã–¥—É—â–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ø–∏–∫–µ—Ä
        for i in range(idx - 1, -1, -1):
            if segments[i]["speaker"] != "Unknown":
                before_speaker = segments[i]["speaker"]
                break
        
        # –°–ª–µ–¥—É—é—â–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ø–∏–∫–µ—Ä  
        for i in range(idx + 1, len(segments)):
            if segments[i]["speaker"] != "Unknown":
                after_speaker = segments[i]["speaker"]
                break
        
        # –ï—Å–ª–∏ –æ–∫—Ä—É–∂–µ–Ω –æ–¥–Ω–∏–º —Å–ø–∏–∫–µ—Ä–æ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if before_speaker and before_speaker == after_speaker:
            return before_speaker
        
        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–ª–∏–∂–∞–π—à–µ–≥–æ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        if before_speaker:
            return before_speaker
        elif after_speaker:
            return after_speaker
        
        return "Unknown"
    
    def process(self, audio_path: str, output_dir: str = "output", 
                min_speakers: int = 1, max_speakers: int = 10, 
                min_segment_duration: float = 0.5, alignment_strategy: str = "smart") -> Dict:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_speakers: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤
            max_speakers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤  
            min_segment_duration: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞
            alignment_strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–≤–º–µ—â–µ–Ω–∏—è ('strict', 'smart', 'aggressive')
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤—ã–≤–æ–¥–∞
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—É–¥–∏–æ
        prepared_audio = self._prepare_audio(audio_path)
        
        try:
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            start_time = time.time()
            transcription_result = self.transcribe(prepared_audio)
            transcription_time = time.time() - start_time
            
            # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            start_time = time.time()
            diarization_result = self.diarize(
                prepared_audio, 
                min_speakers=min_speakers,
                max_speakers=max_speakers,
                min_segment_duration=min_segment_duration
            )
            diarization_time = time.time() - start_time
            
            # –°–æ–≤–º–µ—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
            aligned_segments = self._align_transcription_with_speakers(
                transcription_result, 
                diarization_result,
                alignment_strategy=alignment_strategy
            )
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "audio_file": str(Path(audio_path).name),
                "transcription": transcription_result.get("text", ""),
                "segments": aligned_segments,
                "language": transcription_result.get("language", "unknown"),
                "has_speaker_diarization": diarization_result is not None,
                "transcription_time": transcription_time,
                "diarization_time": diarization_time,
                "diarization_stats": diarization_result.get("stats", {}) if diarization_result else {},
                "alignment_strategy": alignment_strategy
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self._save_results(result, output_path, Path(audio_path).stem)
            
            return result
            
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –æ–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω
            if prepared_audio != audio_path and os.path.exists(prepared_audio):
                os.unlink(prepared_audio)
    
    def _save_results(self, result: Dict, output_path: Path, base_name: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""
        
        # JSON
        json_path = output_path / f"{base_name}_result.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {json_path}")
        
        # CSV
        if result["segments"]:
            df = pd.DataFrame(result["segments"])
            csv_path = output_path / f"{base_name}_segments.csv"
            df.to_csv(csv_path, index=False, encoding='utf-8')
            print(f"üíæ –°–µ–≥–º–µ–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
        
        # TXT (—á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç)
        txt_path = output_path / f"{base_name}_transcript.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {result['audio_file']}\n")
            f.write(f"–Ø–∑—ã–∫: {result['language']}\n")
            f.write(f"–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è: {'–î–∞' if result['has_speaker_diarization'] else '–ù–µ—Ç'}\n\n")
            
            if result["has_speaker_diarization"]:
                f.write("=== –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø –ü–û –°–ü–ò–ö–ï–†–ê–ú ===\n\n")
                current_speaker = None
                for segment in result["segments"]:
                    if segment["speaker"] != current_speaker:
                        current_speaker = segment["speaker"]
                        f.write(f"\n[{current_speaker}]:\n")
                    
                    start_time = f"{int(segment['start']//60):02d}:{int(segment['start']%60):02d}"
                    f.write(f"{start_time} - {segment['text']}\n")
            else:
                f.write("=== –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø ===\n\n")
                f.write(result["transcription"])
        
        print(f"üíæ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {txt_path}")

    def test_transcription_with_different_settings(self, audio_path: str) -> Dict:
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ä–∞–∑–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
        """
        print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏...")
        
        settings_to_test = [
            {
                "name": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏",
                "options": {
                    "language": "ru",
                    "word_timestamps": True
                }
            },
            {
                "name": "–ü—É—Å—Ç–æ–π initial_prompt",
                "options": {
                    "language": "ru",
                    "word_timestamps": True,
                    "initial_prompt": ""
                }
            },
            {
                "name": "–ù–∏–∑–∫–∏–µ –ø–æ—Ä–æ–≥–∏",
                "options": {
                    "language": "ru",
                    "word_timestamps": True,
                    "initial_prompt": "",
                    "no_speech_threshold": 0.2,
                    "logprob_threshold": -1.5,
                    "temperature": 0.1
                }
            },
            {
                "name": "–ë–µ–∑ —É—Å–ª–æ–≤–∏—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞",
                "options": {
                    "language": "ru",
                    "word_timestamps": True,
                    "initial_prompt": "",
                    "condition_on_previous_text": False,
                    "no_speech_threshold": 0.3,
                    "temperature": 0.0
                }
            }
        ]
        
        results = {}
        
        for setting in settings_to_test:
            print(f"üìù –¢–µ—Å—Ç–∏—Ä—É–µ–º: {setting['name']}")
            try:
                result = self.whisper_model.transcribe(audio_path, **setting['options'])
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                segments = result.get('segments', [])
                first_segment_start = segments[0]['start'] if segments else 0
                total_duration = segments[-1]['end'] if segments else 0
                
                results[setting['name']] = {
                    'first_segment_start': first_segment_start,
                    'total_segments': len(segments),
                    'total_duration': total_duration,
                    'text_preview': result.get('text', '')[:100] + '...',
                    'full_result': result
                }
                
                print(f"   ‚úÖ –ü–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å: {first_segment_start:.1f}—Å")
                print(f"   üìä –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                results[setting['name']] = {'error': str(e)}
        
        return results


@click.command()
@click.argument('audio_file', type=click.Path(exists=True))
@click.option('--model', '-m', default='large', 
              type=click.Choice(['tiny', 'base', 'small', 'medium', 'large']),
              help='–ú–æ–¥–µ–ª—å Whisper –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è')
@click.option('--output', '-o', default='output', 
              help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
@click.option('--hf-token', envvar='HUGGINGFACE_TOKEN', 
              help='HuggingFace —Ç–æ–∫–µ–Ω (–º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π HUGGINGFACE_TOKEN)')
@click.option('--local-models', envvar='LOCAL_MODELS_DIR',
              help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (–º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π LOCAL_MODELS_DIR)')
@click.option('--device', default=None, type=click.Choice(['cpu', 'cuda', 'mps']), 
              help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (cpu, cuda, mps)')
@click.option('--min-speakers', default=1, type=int,
              help='–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1)')
@click.option('--max-speakers', default=10, type=int,
              help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
@click.option('--min-segment', default=0.5, type=float,
              help='–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.5)')
@click.option('--alignment-strategy', default='smart', type=click.Choice(['strict', 'smart', 'aggressive']),
              help='–°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–≤–º–µ—â–µ–Ω–∏—è (strict, smart, aggressive)')
@click.option('--test-transcription', is_flag=True,
              help='–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º')
def main(audio_file: str, model: str, output: str, hf_token: Optional[str], 
         local_models: Optional[str], device: Optional[str], min_speakers: int, 
         max_speakers: int, min_segment: float, alignment_strategy: str, test_transcription: bool):
    """
    –ü–∞–π–ø–ª–∞–π–Ω —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    
    AUDIO_FILE: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    
    print("üéµ Whisper + PyAnnote Audio Pipeline (–£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è)")
    print(f"üìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {audio_file}")
    print(f"üß† –ú–æ–¥–µ–ª—å Whisper: {model}")
    print(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output}")
    print(f"üë• –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {min_speakers}-{max_speakers} —Å–ø–∏–∫–µ—Ä–æ–≤, –º–∏–Ω. —Å–µ–≥–º–µ–Ω—Ç {min_segment}—Å")
    
    if local_models:
        print(f"üè† –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑: {local_models}")
    elif not hf_token:
        print("‚ö†Ô∏è  HuggingFace —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
        print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ç–æ–∫–µ–Ω: export HUGGINGFACE_TOKEN=your_token")
        print("üí° –ò–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ: python download_models.py")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        processor = AudioProcessor(
            whisper_model=model, 
            hf_token=hf_token,
            local_models_dir=local_models,
            device=device
        )
        
        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –∑–∞–ø—É—Å–∫–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
        if test_transcription:
            print("\nüß™ –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏")
            print("=" * 50)
            
            test_results = processor.test_transcription_with_different_settings(audio_file)
            
            print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
            print("=" * 50)
            
            for setting_name, result in test_results.items():
                print(f"\nüîß {setting_name}:")
                if 'error' in result:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞: {result['error']}")
                else:
                    print(f"   üïí –ù–∞—á–∞–ª–æ –ø–µ—Ä–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞: {result['first_segment_start']:.1f}—Å")
                    print(f"   üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {result['total_segments']}")
                    print(f"   ‚è±Ô∏è  –û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {result['total_duration']:.1f}—Å")
                    print(f"   üìù –ü—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–∞: {result['text_preview']}")
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            best_start_time = float('inf')
            best_setting = None
            
            for setting_name, result in test_results.items():
                if 'error' not in result and result['first_segment_start'] < best_start_time:
                    best_start_time = result['first_segment_start']
                    best_setting = setting_name
            
            if best_setting:
                print(f"\nüèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: '{best_setting}' (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å {best_start_time:.1f}—Å)")
                
                if best_start_time > 5.0:
                    print("‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å –ø—Ä–æ–ø—É—Å–∫–æ–º –Ω–∞—á–∞–ª–∞ –∞—É–¥–∏–æ!")
                    print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
                    print("   ‚Ä¢ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å model='base' –≤–º–µ—Å—Ç–æ medium/small")
                    print("   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∞—É–¥–∏–æ –≤ –ø–µ—Ä–≤—ã–µ 30 —Å–µ–∫—É–Ω–¥")
                    print("   ‚Ä¢ –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∞—É–¥–∏–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–ª–∏–Ω–Ω—ã—Ö –ø–∞—É–∑ –≤ –Ω–∞—á–∞–ª–µ")
                else:
                    print("‚úÖ –ü—Ä–æ–±–ª–µ–º —Å –ø—Ä–æ–ø—É—Å–∫–æ–º –Ω–∞—á–∞–ª–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
            
            return
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—É–¥–∏–æ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        result = processor.process(
            audio_file, 
            output,
            min_speakers=min_speakers,
            max_speakers=max_speakers,
            min_segment_duration=min_segment,
            alignment_strategy=alignment_strategy
        )
        
        print("\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(result['segments'])}")
        
        if result['has_speaker_diarization']:
            speakers = set(seg['speaker'] for seg in result['segments'])
            unknown_count = sum(1 for seg in result['segments'] if seg['speaker'] == 'Unknown')
            
            print(f"üë• –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(speakers)} ({', '.join(speakers)})")
            print(f"üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–≤–º–µ—â–µ–Ω–∏—è: {result['alignment_strategy']}")
            
            if unknown_count > 0:
                print(f"‚ö†Ô∏è  Unknown —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {unknown_count} ({unknown_count/len(result['segments'])*100:.1f}%)")
            else:
                print("‚úÖ –í—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–≤—è–∑–∞–Ω—ã –∫ —Å–ø–∏–∫–µ—Ä–∞–º!")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            stats = result.get('diarization_stats', {})
            if stats:
                print(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏:")
                print(f"   ‚Ä¢ –°–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {stats.get('segments_before_filter', 'N/A')}")
                print(f"   ‚Ä¢ –°–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {stats.get('segments_after_filter', 'N/A')}")
                print(f"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤: {stats.get('unique_speakers', 'N/A')}")
        
        print(f"üî§ –Ø–∑—ã–∫: {result['language']}")
        print(f"üìù –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(result['transcription'])} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {result['transcription_time']:.1f}—Å")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {result['diarization_time']:.1f}—Å")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 