#!/usr/bin/env python3
"""
Whisper + PyAnnote Audio Pipeline
–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤
"""

import os
import sys
import warnings
import tempfile
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import click
import torch
import whisper
import librosa
import soundfile as sf
from pyannote.audio import Pipeline
from pyannote.core import Segment
import pandas as pd
from tqdm import tqdm
import time

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ HuggingFace
try:
    from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperTokenizer
    from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, pipeline
    HF_TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ö–∞—Å—Ç–æ–º–Ω—ã–µ –º–æ–¥–µ–ª–∏ HF –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
    print("üí° –î–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers")
    HF_TRANSFORMERS_AVAILABLE = False

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings("ignore")


class AudioProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è"""
    
    def __init__(self, whisper_model: str = "base", hf_token: Optional[str] = None, 
                 local_models_dir: Optional[str] = None, device: Optional[str] = None,
                 custom_whisper_model: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            whisper_model: –ú–æ–¥–µ–ª—å Whisper (tiny, base, small, medium, large)
            hf_token: HuggingFace —Ç–æ–∫–µ–Ω –¥–ª—è PyAnnotate
            local_models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (cpu, cuda, mps)
            custom_whisper_model: –ü—É—Ç—å –∫ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏ Whisper (HuggingFace format) –∏–ª–∏ HF model ID
        """
        self.whisper_model_name = whisper_model
        self.custom_whisper_model = custom_whisper_model
        self.hf_token = hf_token
        self.local_models_dir = Path(local_models_dir) if local_models_dir else None
        
        # –¢–∏–ø –º–æ–¥–µ–ª–∏ Whisper (standard –∏–ª–∏ custom)
        self.whisper_model_type = "custom" if custom_whisper_model else "standard"
        
        # –í—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if device is not None:
            self.device = device
        else:
            if torch.cuda.is_available():
                self.device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = "mps"  # Apple Silicon
            else:
                self.device = "cpu"
            
        print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π
        self.whisper_model = None
        self.whisper_processor = None
        self.whisper_pipeline = None  # –î–ª—è pipeline API
        
        if self.custom_whisper_model:
            print(f"üß† –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å Whisper: {self.custom_whisper_model}")
        else:
            print(f"üß† –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–æ–¥–µ–ª—å Whisper: {whisper_model}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
        self._load_models()
    
    def _load_local_config(self) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π"""
        if not self.local_models_dir:
            return None
            
        config_file = self.local_models_dir / "local_config.json"
        if not config_file.exists():
            return None
            
        try:
            with open(config_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return None
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π Whisper –∏ PyAnnotate"""
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Whisper...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ Whisper –º–æ–¥–µ–ª–∏
        whisper_device = self.device
        
        if self.whisper_model_type == "custom" and self.custom_whisper_model:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ transformers
            self._load_custom_whisper_model(whisper_device)
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ whisper
            self._load_standard_whisper_model(whisper_device)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ (–æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏...")
        self.diarization_pipeline = None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        local_config = self._load_local_config()
        
        if local_config and self.local_models_dir:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            try:
                pyannote_model_path = self.local_models_dir / "pyannote" / "speaker-diarization-3.1"
                
                if pyannote_model_path.exists():
                    print(f"üè† –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–∑: {pyannote_model_path}")
                    self.diarization_pipeline = Pipeline.from_pretrained(str(pyannote_model_path))
                    
                    if self.device != "cpu":
                        self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
                    
                    print("‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                    return
                else:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫–µ—à–∞ –±–µ–∑ —Ç–æ–∫–µ–Ω–∞
                    print("üîÑ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –∫–µ—à–∞...")
                    self.diarization_pipeline = Pipeline.from_pretrained(
                        "pyannote/speaker-diarization-3.1"
                    )
                    
                    if self.device != "cpu":
                        self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
                    
                    print("‚úÖ –ú–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫–µ—à–∞")
                    return
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
                print("üîÑ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ HuggingFace...")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫–µ—à–∞ –±–µ–∑ —Ç–æ–∫–µ–Ω–∞ (–µ—Å–ª–∏ –Ω–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)
        try:
            print("üîÑ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –∫–µ—à–∞...")
            self.diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1"
            )
            
            if self.device != "cpu":
                self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
            
            print("‚úÖ –ú–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫–µ—à–∞")
            return
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–µ—à–∞: {e}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ HuggingFace
        try:
            if not self.hf_token:
                print("‚ö†Ô∏è  HuggingFace —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")
                print("üí° –î–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏")
                print("üí° –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏: python download_models.py")
                return
                
            self.diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=self.hf_token
            )
            
            if self.device != "cpu":
                self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
                
            print("‚úÖ –ú–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ HuggingFace")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å HuggingFace —Ç–æ–∫–µ–Ω –∏ –¥–æ—Å—Ç—É–ø –∫ pyannote/speaker-diarization-3.1")
            print("üí° –ò–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ: python download_models.py")
            self.diarization_pipeline = None
    
    def _load_standard_whisper_model(self, whisper_device: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ Whisper"""
        try:
            self.whisper_model = whisper.load_model(
                self.whisper_model_name, 
                device=whisper_device
            )
            self.whisper_processor = None  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç processor
            print(f"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–æ–¥–µ–ª—å Whisper –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {whisper_device}")
        except Exception as e:
            if whisper_device == "mps" and ("SparseMPS" in str(e) or "aten::empty.memory_format" in str(e) or "_sparse_coo_tensor_with_dims_and_tensors" in str(e)):
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Whisper –Ω–∞ MPS: {e}")
                print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU –¥–ª—è Whisper...")
                whisper_device = "cpu"
                self.whisper_model = whisper.load_model(
                    self.whisper_model_name, 
                    device=whisper_device
                )
                print("‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–æ–¥–µ–ª—å Whisper –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")
            else:
                raise e
    
    def _load_custom_whisper_model(self, whisper_device: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏ Whisper —á–µ—Ä–µ–∑ transformers —Å pipeline API"""
        if not HF_TRANSFORMERS_AVAILABLE:
            raise ImportError("transformers –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        
        try:
            print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å Whisper: {self.custom_whisper_model}")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            torch_dtype = torch.bfloat16 if whisper_device != "cpu" else torch.float32
            
            # Monkey patching –¥–ª—è MPS –∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            if whisper_device == "mps" and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                try:
                    setattr(torch.distributed, "is_initialized", lambda: False)
                    print("üîß –ü—Ä–∏–º–µ–Ω–µ–Ω monkey patch –¥–ª—è MPS")
                except Exception as e:
                    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å monkey patch –¥–ª—è MPS: {e}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            model_kwargs = {
                "torch_dtype": torch_dtype,
                "low_cpu_mem_usage": True,
                "use_safetensors": True
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º flash attention –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ (–¥–ª—è CUDA)
            if whisper_device == "cuda":
                try:
                    model_kwargs["attn_implementation"] = "flash_attention_2"
                    print("üöÄ –í–∫–ª—é—á–µ–Ω flash_attention_2")
                except Exception:
                    print("‚ö†Ô∏è  flash_attention_2 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ")
            
            if Path(self.custom_whisper_model).exists():
                # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
                print("üè† –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å...")
                self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
                    self.custom_whisper_model,
                    **model_kwargs
                )
                self.whisper_processor = WhisperProcessor.from_pretrained(self.custom_whisper_model)
            else:
                # –ú–æ–¥–µ–ª—å –∏–∑ HuggingFace Hub
                print("üåê –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ HuggingFace Hub...")
                self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
                    self.custom_whisper_model,
                    **model_kwargs
                )
                self.whisper_processor = WhisperProcessor.from_pretrained(self.custom_whisper_model)
            
            # –°–æ–∑–¥–∞–µ–º pipeline –∫–∞–∫ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            print("üîÑ –°–æ–∑–¥–∞–µ–º ASR pipeline...")
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º device –¥–ª—è pipeline
            pipeline_device = whisper_device
            if whisper_device == "mps":
                # –î–ª—è MPS –∏—Å–ø–æ–ª—å–∑—É–µ–º device index
                pipeline_device = torch.device(whisper_device)
            
            self.whisper_pipeline = pipeline(
                "automatic-speech-recognition",
                model=self.whisper_model,
                tokenizer=self.whisper_processor.tokenizer,
                feature_extractor=self.whisper_processor.feature_extractor,
                max_new_tokens=256,
                chunk_length_s=30,
                batch_size=16,
                return_timestamps=True,
                torch_dtype=torch_dtype,
                device=pipeline_device
            )
            
            print(f"‚úÖ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å Whisper –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å pipeline API")
            
        except Exception as e:
            if whisper_device == "mps" and ("MPS" in str(e) or "SparseMPS" in str(e)):
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ MPS: {e}")
                print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏...")
                whisper_device = "cpu"
                self._load_custom_whisper_model(whisper_device)
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
                print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–æ–¥–µ–ª—å...")
                self.whisper_model_type = "standard"
                self.custom_whisper_model = None
                self.whisper_pipeline = None
                self._load_standard_whisper_model(whisper_device)
    
    def _prepare_audio(self, audio_path: str) -> str:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            
        Returns:
            –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
        """
        audio_path = Path(audio_path)
        
        if not audio_path.exists():
            raise FileNotFoundError(f"–ê—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")
        
        # –ï—Å–ª–∏ —É–∂–µ wav —Ñ–∞–π–ª, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if audio_path.suffix.lower() == '.wav':
            return str(audio_path)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ wav
        print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –≤ WAV —Ñ–æ—Ä–º–∞—Ç...")
        audio, sr = librosa.load(str(audio_path), sr=16000)
        
        temp_wav = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
        sf.write(temp_wav.name, audio, sr)
        
        return temp_wav.name
    
    def transcribe(self, audio_path: str, time_limit: Optional[float] = None) -> Dict:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é Whisper
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            time_limit: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        """
        print("üé§ –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é...")
        
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –æ–±—Ä–µ–∑–∞–µ–º –∞—É–¥–∏–æ
        if time_limit is not None:
            print(f"‚è±Ô∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏: {time_limit} —Å–µ–∫—É–Ω–¥")
            audio, sr = librosa.load(audio_path, sr=16000)
            max_samples = int(time_limit * sr)
            if len(audio) > max_samples:
                audio = audio[:max_samples]
                temp_wav = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
                sf.write(temp_wav.name, audio, sr)
                audio_path = temp_wav.name
                print(f"‚úÇÔ∏è  –ê—É–¥–∏–æ –æ–±—Ä–µ–∑–∞–Ω–æ –¥–æ {time_limit} —Å–µ–∫—É–Ω–¥")
        
        # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
        if self.whisper_model_type == "custom" and self.whisper_pipeline is not None:
            result = self._transcribe_with_pipeline(audio_path)
        elif self.whisper_model_type == "custom" and self.whisper_processor is not None:
            result = self._transcribe_with_custom_model(audio_path)
        else:
            result = self._transcribe_with_standard_model(audio_path)
        
        # –ï—Å–ª–∏ –º—ã —Å–æ–∑–¥–∞–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, —É–¥–∞–ª—è–µ–º –µ–≥–æ
        if time_limit is not None and audio_path != audio_path:
            os.unlink(audio_path)
        
        return result
    
    def _transcribe_with_standard_model(self, audio_path: str) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –º–æ–¥–µ–ª—å—é Whisper"""
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–∞ –Ω–∞—á–∞–ª–∞ –∞—É–¥–∏–æ
        transcribe_options = {
            "language": "ru",
            "word_timestamps": True,
            "initial_prompt": "",  # –ü—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–∞
            "temperature": 0.0,    # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            "no_speech_threshold": 0.4,  # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–µ—á–∏
            "logprob_threshold": -1.0,   # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            "compression_ratio_threshold": 2.4,  # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–∞
        }
        
        # –î–ª—è –º–æ–¥–µ–ª–µ–π small –∏ medium –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        if self.whisper_model_name in ['small', 'medium', 'large']:
            print("üîß –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è small/medium/large –º–æ–¥–µ–ª–∏...")
            transcribe_options.update({
                "temperature": 0.1,    # –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É
                "no_speech_threshold": 0.3,  # –ï—â–µ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥
                "condition_on_previous_text": False,  # –û—Ç–∫–ª—é—á–∞–µ–º —É—Å–ª–æ–≤–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            })
        
        result = self.whisper_model.transcribe(
            audio_path,
            **transcribe_options
        )
        
        return result
    
    def _transcribe_with_custom_model(self, audio_path: str) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª—å—é —á–µ—Ä–µ–∑ transformers"""
        import librosa
        
        print("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        audio, sr = librosa.load(audio_path, sr=16000)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        inputs = self.whisper_processor(
            audio, 
            sampling_rate=16000, 
            return_tensors="pt"
        )
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        if hasattr(self.whisper_model, 'device') and self.whisper_model.device != torch.device('cpu'):
            try:
                inputs = {k: v.to(self.whisper_model.device) for k, v in inputs.items()}
            except Exception as e:
                print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ {self.whisper_model.device}: {e}")
                print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        generate_kwargs = {
            "language": "russian",
            "task": "transcribe",
            "return_timestamps": True,
            "max_new_tokens": 256,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 448 –¥–æ 256 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤
            "do_sample": False,  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
            "num_beams": 1,      # Greedy search
        }
        
        # –£–±–∏—Ä–∞–µ–º return_timestamps –µ—Å–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
            with torch.no_grad():
                predicted_ids = self.whisper_model.generate(
                    inputs["input_features"],
                    **generate_kwargs
                )
        except Exception as e:
            if "return_timestamps" in str(e):
                print("‚ö†Ô∏è  return_timestamps –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫")
                generate_kwargs.pop("return_timestamps", None)
                with torch.no_grad():
                    predicted_ids = self.whisper_model.generate(
                        inputs["input_features"],
                        **generate_kwargs
                    )
            else:
                raise e
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        transcription = self.whisper_processor.batch_decode(
            predicted_ids, 
            skip_special_tokens=True
        )[0]
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        detailed_result = self._get_detailed_transcription_custom(
            audio_path, inputs, predicted_ids, transcription
        )
        
        return detailed_result
    
    def _get_detailed_transcription_custom(self, audio_path: str, inputs: Dict, 
                                         predicted_ids: torch.Tensor, full_text: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        # –ü–æ–∫–∞ —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —Å–æ–≤–º–µ—Å—Ç–∏–º—É—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –º–æ–¥–µ–ª—å—é
        # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
        
        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ –¥–ª–∏–Ω–µ –∞—É–¥–∏–æ
        audio, sr = librosa.load(audio_path, sr=16000)
        audio_duration = len(audio) / sr
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ —Å–ª–æ–≤–∞–º/–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        sentences = full_text.split('. ')
        segments = []
        
        segment_duration = audio_duration / len(sentences) if sentences else audio_duration
        
        for i, sentence in enumerate(sentences):
            if sentence.strip():
                start_time = i * segment_duration
                end_time = min((i + 1) * segment_duration, audio_duration)
                
                segments.append({
                    "start": start_time,
                    "end": end_time,
                    "text": sentence.strip() + ('.' if i < len(sentences) - 1 else '')
                })
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–æ–≤, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω
        if not segments:
            segments = [{
                "start": 0.0,
                "end": audio_duration,
                "text": full_text
            }]
        
        return {
            "text": full_text,
            "segments": segments,
            "language": "ru"
        }
    
    def _transcribe_with_pipeline(self, audio_path: str) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pipeline API (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)"""
        print("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º pipeline API –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –∫–∞–∫ numpy array (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è pipeline)
            import librosa
            
            print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ —Ñ–∞–π–ª...")
            audio, sr = librosa.load(audio_path, sr=16000)
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–∫ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            generate_kwargs = {
                "language": "russian",
                "max_new_tokens": 256
            }
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —á–µ—Ä–µ–∑ pipeline
            print("üé§ –í—ã–ø–æ–ª–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —á–µ—Ä–µ–∑ pipeline...")
            pipeline_result = self.whisper_pipeline(
                audio,  # –ü–µ—Ä–µ–¥–∞–µ–º numpy array –≤–º–µ—Å—Ç–æ BytesIO
                generate_kwargs=generate_kwargs,
                return_timestamps=True
            )
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç pipeline –≤ —Ñ–æ—Ä–º–∞—Ç, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –º–æ–¥–µ–ª—å—é
            result = self._convert_pipeline_result_to_standard_format(pipeline_result, audio_path)
            
            return result
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —á–µ—Ä–µ–∑ pipeline: {e}")
            print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥...")
            # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏
            return self._transcribe_with_custom_model(audio_path)
    
    def _convert_pipeline_result_to_standard_format(self, pipeline_result: Dict, audio_path: str) -> Dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç pipeline –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        # Pipeline –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
        # {"text": "...", "chunks": [{"timestamp": (start, end), "text": "..."}]}
        
        text = pipeline_result.get("text", "")
        chunks = pipeline_result.get("chunks", [])
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ Whisper
        segments = []
        
        if chunks:
            for chunk in chunks:
                timestamp = chunk.get("timestamp")
                chunk_text = chunk.get("text", "")
                
                if timestamp and len(timestamp) >= 2:
                    start_time = timestamp[0] if timestamp[0] is not None else 0.0
                    end_time = timestamp[1] if timestamp[1] is not None else start_time + 1.0
                    
                    segments.append({
                        "start": start_time,
                        "end": end_time,
                        "text": chunk_text.strip()
                    })
        
        # –ï—Å–ª–∏ –Ω–µ—Ç chunks, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç –∏–∑ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
        if not segments and text:
            # –ü–æ–ª—É—á–∞–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ
            try:
                import librosa
                audio, sr = librosa.load(audio_path, sr=16000)
                audio_duration = len(audio) / sr
            except Exception:
                audio_duration = 30.0  # Fallback
            
            segments = [{
                "start": 0.0,
                "end": audio_duration,
                "text": text.strip()
            }]
        
        return {
            "text": text,
            "segments": segments,
            "language": "ru"
        }
    
    def diarize(self, audio_path: str, min_speakers: int = 1, max_speakers: int = 10, 
                min_segment_duration: float = 0.5) -> Optional[Dict]:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            min_speakers: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤
            max_speakers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤
            min_segment_duration: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞ (—Å–µ–∫)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ None –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
        """
        if self.diarization_pipeline is None:
            print("‚ö†Ô∏è  –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ - –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return None
            
        print("üë• –ù–∞—á–∏–Ω–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å–ø–∏–∫–µ—Ä–æ–≤...")
        print(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: —Å–ø–∏–∫–µ—Ä—ã {min_speakers}-{max_speakers}, –º–∏–Ω. —Å–µ–≥–º–µ–Ω—Ç {min_segment_duration}—Å")
        
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            diarization_params = {
                "min_speakers": min_speakers,
                "max_speakers": max_speakers
            }
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            print("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞—É–¥–∏–æ...")
            diarization = self.diarization_pipeline(audio_path, **diarization_params)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            speakers = []
            segment_count_before = 0
            
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                segment_count_before += 1
                duration = turn.end - turn.start
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã
                if duration >= min_segment_duration:
                    speakers.append({
                        "start": turn.start,
                        "end": turn.end,
                        "speaker": speaker,
                        "duration": duration
                    })
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            speakers = self._merge_consecutive_same_speaker(speakers)
            
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤ –≤ –ø–æ–Ω—è—Ç–Ω—ã–µ –∏–º–µ–Ω–∞
            speakers = self._rename_speakers(speakers)
            
            print(f"üìä –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {segment_count_before} ‚Üí {len(speakers)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            print(f"üë• –ù–∞–π–¥–µ–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(set(s['speaker'] for s in speakers))}")
            
            return {
                "speakers": speakers,
                "stats": {
                    "segments_before_filter": segment_count_before,
                    "segments_after_filter": len(speakers),
                    "unique_speakers": len(set(s['speaker'] for s in speakers))
                }
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return None
    
    def _merge_consecutive_same_speaker(self, speakers: List[Dict], gap_threshold: float = 0.3) -> List[Dict]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø–∞—É–∑–∞–º–∏
        
        Args:
            speakers: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å–ø–∏–∫–µ—Ä–æ–≤
            gap_threshold: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è (—Å–µ–∫)
        """
        if not speakers:
            return speakers
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞
        speakers = sorted(speakers, key=lambda x: x['start'])
        merged = [speakers[0]]
        
        for current in speakers[1:]:
            last = merged[-1]
            
            # –ï—Å–ª–∏ —Ç–æ—Ç –∂–µ —Å–ø–∏–∫–µ—Ä –∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ –Ω–µ–±–æ–ª—å—à–æ–π - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
            if (current['speaker'] == last['speaker'] and 
                current['start'] - last['end'] <= gap_threshold):
                
                # –†–∞—Å—à–∏—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
                merged[-1] = {
                    "start": last['start'],
                    "end": current['end'],
                    "speaker": last['speaker'],
                    "duration": current['end'] - last['start']
                }
            else:
                merged.append(current)
        
        return merged
    
    def _rename_speakers(self, speakers: List[Dict]) -> List[Dict]:
        """
        –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ—Ç —Å–ø–∏–∫–µ—Ä–æ–≤ –≤ –ø–æ–Ω—è—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ (–°–ø–∏–∫–µ—Ä 1, –°–ø–∏–∫–µ—Ä 2, etc.)
        """
        if not speakers:
            return speakers
        
        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ –ø–æ—è–≤–ª–µ–Ω–∏—è
        unique_speakers = []
        for speaker_data in speakers:
            speaker = speaker_data['speaker']
            if speaker not in unique_speakers:
                unique_speakers.append(speaker)
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ —Å—Ç–∞—Ä—ã—Ö –∏–º–µ–Ω –Ω–∞ –Ω–æ–≤—ã–µ
        speaker_mapping = {}
        for i, old_name in enumerate(unique_speakers):
            speaker_mapping[old_name] = f"–°–ø–∏–∫–µ—Ä {i + 1}"
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ–≤—ã–µ –∏–º–µ–Ω–∞
        for speaker_data in speakers:
            speaker_data['speaker'] = speaker_mapping[speaker_data['speaker']]
        
        return speakers
    
    def _align_transcription_with_speakers(self, transcription: Dict, diarization: Dict, 
                                          alignment_strategy: str = "smart") -> List[Dict]:
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–æ–≤–º–µ—â–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–ø–∏–∫–µ—Ä–∞—Ö
        
        Args:
            transcription: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            diarization: –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            alignment_strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–≤–º–µ—â–µ–Ω–∏—è ('strict', 'smart', 'aggressive')
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ —Å–ø–∏–∫–µ—Ä–∞–º–∏
        """
        if not diarization or "speakers" not in diarization:
            # –ï—Å–ª–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
            segments = []
            for segment in transcription.get("segments", []):
                segments.append({
                    "start": segment["start"],
                    "end": segment["end"],
                    "text": segment["text"].strip(),
                    "speaker": "Unknown"
                })
            return segments
        
        # –°–æ–≤–º–µ—â–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é
        aligned_segments = []
        speaker_segments = diarization["speakers"]
        transcription_segments = transcription.get("segments", [])
        
        print(f"üîÑ –°–æ–≤–º–µ—â–µ–Ω–∏–µ —Å —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π '{alignment_strategy}'...")
        
        for t_segment in transcription_segments:
            t_start, t_end = t_segment["start"], t_segment["end"]
            t_mid = (t_start + t_end) / 2
            
            best_speaker = self._find_best_speaker(
                t_start, t_end, t_mid, speaker_segments, alignment_strategy
            )
            
            aligned_segments.append({
                "start": t_start,
                "end": t_end,
                "text": t_segment["text"].strip(),
                "speaker": best_speaker
            })
        
        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: —É—Å—Ç—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è Unknown —Å–µ–≥–º–µ–Ω—Ç—ã
        aligned_segments = self._resolve_unknown_speakers(aligned_segments, speaker_segments)
        
        return aligned_segments
    
    def _find_best_speaker(self, t_start: float, t_end: float, t_mid: float, 
                          speaker_segments: List[Dict], strategy: str) -> str:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        """
        best_speaker = "Unknown"
        best_score = 0
        
        for s_segment in speaker_segments:
            s_start, s_end = s_segment["start"], s_segment["end"]
            score = 0
            
            if strategy == "strict":
                # –¢–æ–ª—å–∫–æ —Å—Ç—Ä–æ–≥–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
                overlap_start = max(t_start, s_start)
                overlap_end = min(t_end, s_end)
                overlap = max(0, overlap_end - overlap_start)
                score = overlap
                
            elif strategy == "smart":
                # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
                # 1. –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
                overlap_start = max(t_start, s_start)
                overlap_end = min(t_end, s_end)
                overlap = max(0, overlap_end - overlap_start)
                
                if overlap > 0:
                    score = overlap * 10  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è–º
                else:
                    # 2. –ë–ª–∏–∑–æ—Å—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                    gap_to_start = abs(t_mid - s_start)
                    gap_to_end = abs(t_mid - s_end)
                    min_gap = min(gap_to_start, gap_to_end)
                    
                    # 3. –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
                    s_mid = (s_start + s_end) / 2
                    gap_to_mid = abs(t_mid - s_mid)
                    
                    # –ß–µ–º –±–ª–∏–∂–µ, —Ç–µ–º –≤—ã—à–µ —Å—á–µ—Ç (–Ω–æ –º–µ–Ω—å—à–µ —á–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ)
                    if min_gap < 2.0:  # –ú–∞–∫—Å–∏–º—É–º 2 —Å–µ–∫—É–Ω–¥—ã —Ä–∞–∑—Ä—ã–≤–∞
                        score = max(0, 2.0 - min_gap)
                    elif gap_to_mid < 3.0:  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –ø–æ —Å—Ä–µ–¥–Ω–µ–π —Ç–æ—á–∫–µ
                        score = max(0, 1.0 - gap_to_mid / 3.0)
                        
            elif strategy == "aggressive":
                # –û—á–µ–Ω—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–æ–≤–º–µ—â–µ–Ω–∏–µ
                # –õ—é–±–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∏–ª–∏ –±–ª–∏–∑–æ—Å—Ç—å
                overlap_start = max(t_start, s_start)
                overlap_end = min(t_end, s_end)
                overlap = max(0, overlap_end - overlap_start)
                
                if overlap > 0:
                    score = overlap * 10
                else:
                    # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–µ–π —Ç–æ—á–∫–∏
                    distances = [
                        abs(t_start - s_start), abs(t_start - s_end),
                        abs(t_end - s_start), abs(t_end - s_end),
                        abs(t_mid - s_start), abs(t_mid - s_end)
                    ]
                    min_distance = min(distances)
                    
                    if min_distance < 5.0:  # –î–æ 5 —Å–µ–∫—É–Ω–¥ —Ä–∞–∑—Ä—ã–≤–∞
                        score = max(0, 5.0 - min_distance)
            
            if score > best_score:
                best_score = score
                best_speaker = s_segment["speaker"]
        
        return best_speaker
    
    def _resolve_unknown_speakers(self, segments: List[Dict], speaker_segments: List[Dict]) -> List[Dict]:
        """
        –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è Unknown —Å–ø–∏–∫–µ—Ä–æ–≤
        """
        if not speaker_segments:
            return segments
            
        # –°–æ–±–∏—Ä–∞–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤
        known_speakers = [s["speaker"] for s in speaker_segments]
        unknown_segments = [i for i, seg in enumerate(segments) if seg["speaker"] == "Unknown"]
        
        if not unknown_segments:
            return segments
            
        print(f"üîß –ò—Å–ø—Ä–∞–≤–ª—è–µ–º {len(unknown_segments)} Unknown —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
        
        for idx in unknown_segments:
            segment = segments[idx]
            t_start, t_end = segment["start"], segment["end"]
            t_mid = (t_start + t_end) / 2
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ë–ª–∏–∂–∞–π—à–∏–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ø–∏–∫–µ—Ä –∏–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            best_speaker = self._find_nearest_speaker(t_mid, speaker_segments)
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            if best_speaker == "Unknown":
                best_speaker = self._find_contextual_speaker(idx, segments, known_speakers)
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –°–∞–º—ã–π —á–∞—Å—Ç—ã–π —Å–ø–∏–∫–µ—Ä –≤ –∞—É–¥–∏–æ
            if best_speaker == "Unknown" and known_speakers:
                speaker_counts = {}
                for s in segments:
                    if s["speaker"] != "Unknown":
                        speaker_counts[s["speaker"]] = speaker_counts.get(s["speaker"], 0) + 1
                
                if speaker_counts:
                    best_speaker = max(speaker_counts, key=speaker_counts.get)
                else:
                    best_speaker = known_speakers[0] if known_speakers else "–°–ø–∏–∫–µ—Ä 1"
            
            segments[idx]["speaker"] = best_speaker
        
        return segments
    
    def _find_nearest_speaker(self, t_mid: float, speaker_segments: List[Dict]) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç –±–ª–∏–∂–∞–π—à–µ–≥–æ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–ø–∏–∫–µ—Ä–∞"""
        min_distance = float('inf')
        nearest_speaker = "Unknown"
        
        for s_segment in speaker_segments:
            s_start, s_end = s_segment["start"], s_segment["end"]
            s_mid = (s_start + s_end) / 2
            
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Å—Ä–µ–¥–Ω–µ–π —Ç–æ—á–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            distance = abs(t_mid - s_mid)
            
            if distance < min_distance:
                min_distance = distance
                nearest_speaker = s_segment["speaker"]
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ —Ä–∞–∑—É–º–Ω–æ–µ (< 10 —Å–µ–∫—É–Ω–¥)
        return nearest_speaker if min_distance < 10.0 else "Unknown"
    
    def _find_contextual_speaker(self, idx: int, segments: List[Dict], known_speakers: List[str]) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–ø–∏–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
        # –°–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        before_speaker = None
        after_speaker = None
        
        # –ü—Ä–µ–¥—ã–¥—É—â–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ø–∏–∫–µ—Ä
        for i in range(idx - 1, -1, -1):
            if segments[i]["speaker"] != "Unknown":
                before_speaker = segments[i]["speaker"]
                break
        
        # –°–ª–µ–¥—É—é—â–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ø–∏–∫–µ—Ä  
        for i in range(idx + 1, len(segments)):
            if segments[i]["speaker"] != "Unknown":
                after_speaker = segments[i]["speaker"]
                break
        
        # –ï—Å–ª–∏ –æ–∫—Ä—É–∂–µ–Ω –æ–¥–Ω–∏–º —Å–ø–∏–∫–µ—Ä–æ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if before_speaker and before_speaker == after_speaker:
            return before_speaker
        
        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–ª–∏–∂–∞–π—à–µ–≥–æ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        if before_speaker:
            return before_speaker
        elif after_speaker:
            return after_speaker
        
        return "Unknown"
    
    def process(self, audio_path: str, output_dir: str = "output", 
                min_speakers: int = 1, max_speakers: int = 10, 
                min_segment_duration: float = 0.5, alignment_strategy: str = "smart",
                time_limit: Optional[float] = None) -> Dict:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_speakers: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤
            max_speakers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤  
            min_segment_duration: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞
            alignment_strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–≤–º–µ—â–µ–Ω–∏—è ('strict', 'smart', 'aggressive')
            time_limit: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤—ã–≤–æ–¥–∞
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—É–¥–∏–æ
        prepared_audio = self._prepare_audio(audio_path)
        
        try:
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            start_time = time.time()
            transcription_result = self.transcribe(prepared_audio, time_limit=time_limit)
            transcription_time = time.time() - start_time
            
            # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            start_time = time.time()
            diarization_result = self.diarize(
                prepared_audio, 
                min_speakers=min_speakers,
                max_speakers=max_speakers,
                min_segment_duration=min_segment_duration
            )
            diarization_time = time.time() - start_time
            
            # –°–æ–≤–º–µ—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
            aligned_segments = self._align_transcription_with_speakers(
                transcription_result, 
                diarization_result,
                alignment_strategy=alignment_strategy
            )
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "audio_file": str(Path(audio_path).name),
                "transcription": transcription_result.get("text", ""),
                "segments": aligned_segments,
                "language": transcription_result.get("language", "unknown"),
                "has_speaker_diarization": diarization_result is not None,
                "transcription_time": transcription_time,
                "diarization_time": diarization_time,
                "diarization_stats": diarization_result.get("stats", {}) if diarization_result else {},
                "alignment_strategy": alignment_strategy
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self._save_results(result, output_path, Path(audio_path).stem)
            
            return result
            
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –æ–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω
            if prepared_audio != audio_path and os.path.exists(prepared_audio):
                os.unlink(prepared_audio)
    
    def _save_results(self, result: Dict, output_path: Path, base_name: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""
        
        # JSON
        json_path = output_path / f"{base_name}_result.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {json_path}")
        
        # CSV
        if result["segments"]:
            df = pd.DataFrame(result["segments"])
            csv_path = output_path / f"{base_name}_segments.csv"
            df.to_csv(csv_path, index=False, encoding='utf-8')
            print(f"üíæ –°–µ–≥–º–µ–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
        
        # TXT (—á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç)
        txt_path = output_path / f"{base_name}_transcript.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {result['audio_file']}\n")
            f.write(f"–Ø–∑—ã–∫: {result['language']}\n")
            f.write(f"–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è: {'–î–∞' if result['has_speaker_diarization'] else '–ù–µ—Ç'}\n\n")
            
            if result["has_speaker_diarization"]:
                f.write("=== –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø –ü–û –°–ü–ò–ö–ï–†–ê–ú ===\n\n")
                current_speaker = None
                for segment in result["segments"]:
                    if segment["speaker"] != current_speaker:
                        current_speaker = segment["speaker"]
                        f.write(f"\n[{current_speaker}]:\n")
                    
                    start_time = f"{int(segment['start']//60):02d}:{int(segment['start']%60):02d}"
                    f.write(f"{start_time} - {segment['text']}\n")
            else:
                f.write("=== –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø ===\n\n")
                f.write(result["transcription"])
        
        print(f"üíæ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {txt_path}")

    def test_transcription_with_different_settings(self, audio_path: str) -> Dict:
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ä–∞–∑–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
        """
        print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏...")
        
        settings_to_test = [
            {
                "name": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏",
                "options": {
                    "language": "ru",
                    "word_timestamps": True
                }
            },
            {
                "name": "–ü—É—Å—Ç–æ–π initial_prompt",
                "options": {
                    "language": "ru",
                    "word_timestamps": True,
                    "initial_prompt": ""
                }
            },
            {
                "name": "–ù–∏–∑–∫–∏–µ –ø–æ—Ä–æ–≥–∏",
                "options": {
                    "language": "ru",
                    "word_timestamps": True,
                    "initial_prompt": "",
                    "no_speech_threshold": 0.2,
                    "logprob_threshold": -1.5,
                    "temperature": 0.1
                }
            },
            {
                "name": "–ë–µ–∑ —É—Å–ª–æ–≤–∏—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞",
                "options": {
                    "language": "ru",
                    "word_timestamps": True,
                    "initial_prompt": "",
                    "condition_on_previous_text": False,
                    "no_speech_threshold": 0.3,
                    "temperature": 0.0
                }
            }
        ]
        
        results = {}
        
        for setting in settings_to_test:
            print(f"üìù –¢–µ—Å—Ç–∏—Ä—É–µ–º: {setting['name']}")
            try:
                result = self.whisper_model.transcribe(audio_path, **setting['options'])
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                segments = result.get('segments', [])
                first_segment_start = segments[0]['start'] if segments else 0
                total_duration = segments[-1]['end'] if segments else 0
                
                results[setting['name']] = {
                    'first_segment_start': first_segment_start,
                    'total_segments': len(segments),
                    'total_duration': total_duration,
                    'text_preview': result.get('text', '')[:100] + '...',
                    'full_result': result
                }
                
                print(f"   ‚úÖ –ü–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å: {first_segment_start:.1f}—Å")
                print(f"   üìä –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                results[setting['name']] = {'error': str(e)}
        
        return results


@click.command()
@click.argument('audio_file')
@click.option('--model', '-m', default='large', 
              type=click.Choice(['tiny', 'base', 'small', 'medium', 'large']),
              help='–ú–æ–¥–µ–ª—å Whisper –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (—Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)')
@click.option('--custom-model', '--custom-whisper-model', 
              help='–ü—É—Ç—å –∫ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏ Whisper (HuggingFace format) –∏–ª–∏ HF model ID')
@click.option('--output', '-o', default='output', 
              help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
@click.option('--hf-token', envvar='HUGGINGFACE_TOKEN', 
              help='HuggingFace —Ç–æ–∫–µ–Ω (–º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π HUGGINGFACE_TOKEN)')
@click.option('--local-models', envvar='LOCAL_MODELS_DIR',
              help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (–º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π LOCAL_MODELS_DIR)')
@click.option('--device', default=None, type=click.Choice(['cpu', 'cuda', 'mps']), 
              help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (cpu, cuda, mps)')
@click.option('--min-speakers', default=1, type=int,
              help='–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1)')
@click.option('--max-speakers', default=10, type=int,
              help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
@click.option('--min-segment', default=0.5, type=float,
              help='–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.5)')
@click.option('--alignment-strategy', default='smart', type=click.Choice(['strict', 'smart', 'aggressive']),
              help='–°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–≤–º–µ—â–µ–Ω–∏—è (strict, smart, aggressive)')
@click.option('--test-transcription', is_flag=True,
              help='–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º')
@click.option('--time-limit', type=float,
              help='–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, 3500 –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –ø–µ—Ä–≤—ã—Ö 3500 —Å–µ–∫—É–Ω–¥)')
def main(audio_file: str, model: str, custom_model: Optional[str], output: str, hf_token: Optional[str], 
         local_models: Optional[str], device: Optional[str], min_speakers: int, 
         max_speakers: int, min_segment: float, alignment_strategy: str, test_transcription: bool,
         time_limit: Optional[float]):
    """
    –ü–∞–π–ø–ª–∞–π–Ω —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    
    AUDIO_FILE: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
    input_dir = Path("input")
    if input_dir.exists() and not Path(audio_file).exists():
        # –ò—â–µ–º —Ñ–∞–π–ª –≤ input/ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        input_file_path = input_dir / audio_file
        if input_file_path.exists():
            audio_file = str(input_file_path)
        else:
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_file}")
            print(f"üîç –ò—Å–∫–∞–ª –≤: {Path(audio_file).absolute()}")
            print(f"üîç –ò—Å–∫–∞–ª –≤: {input_file_path.absolute()}")
            
            # –ü–æ–∫–∞–∂–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ input/
            if input_dir.exists():
                input_files = list(input_dir.glob("*"))
                if input_files:
                    print(f"üìÅ –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ input/:")
                    for f in input_files:
                        if f.is_file():
                            print(f"   ‚Ä¢ {f.name}")
                else:
                    print(f"üìÅ –ü–∞–ø–∫–∞ input/ –ø—É—Å—Ç–∞")
            sys.exit(1)
    elif not Path(audio_file).exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_file}")
        sys.exit(1)
    
    print("üéµ Whisper + PyAnnote Audio Pipeline (–£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è + –ö–∞—Å—Ç–æ–º–Ω—ã–µ –º–æ–¥–µ–ª–∏)")
    print(f"üìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {audio_file}")
    
    if custom_model:
        print(f"üß† –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å Whisper: {custom_model}")
    else:
        print(f"üß† –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–æ–¥–µ–ª—å Whisper: {model}")
    
    print(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output}")
    print(f"üë• –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {min_speakers}-{max_speakers} —Å–ø–∏–∫–µ—Ä–æ–≤, –º–∏–Ω. —Å–µ–≥–º–µ–Ω—Ç {min_segment}—Å")
    
    if time_limit:
        print(f"‚è±Ô∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏: {time_limit} —Å–µ–∫—É–Ω–¥")
    
    if local_models:
        print(f"üè† –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑: {local_models}")
    elif not hf_token:
        print("‚ö†Ô∏è  HuggingFace —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
        print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ç–æ–∫–µ–Ω: export HUGGINGFACE_TOKEN=your_token")
        print("üí° –ò–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ: python download_models.py")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –æ–ø—Ü–∏–π
    if custom_model and not HF_TRANSFORMERS_AVAILABLE:
        print("‚ùå –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω—É–∂–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ transformers")
        print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers")
        sys.exit(1)
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        processor = AudioProcessor(
            whisper_model=model, 
            hf_token=hf_token,
            local_models_dir=local_models,
            device=device,
            custom_whisper_model=custom_model
        )
        
        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –∑–∞–ø—É—Å–∫–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
        if test_transcription:
            print("\nüß™ –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏")
            print("=" * 50)
            
            test_results = processor.test_transcription_with_different_settings(audio_file)
            
            print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
            print("=" * 50)
            
            for setting_name, result in test_results.items():
                print(f"\nüîß {setting_name}:")
                if 'error' in result:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞: {result['error']}")
                else:
                    print(f"   üïí –ù–∞—á–∞–ª–æ –ø–µ—Ä–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞: {result['first_segment_start']:.1f}—Å")
                    print(f"   üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {result['total_segments']}")
                    print(f"   ‚è±Ô∏è  –û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {result['total_duration']:.1f}—Å")
                    print(f"   üìù –ü—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–∞: {result['text_preview']}")
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            best_start_time = float('inf')
            best_setting = None
            
            for setting_name, result in test_results.items():
                if 'error' not in result and result['first_segment_start'] < best_start_time:
                    best_start_time = result['first_segment_start']
                    best_setting = setting_name
            
            if best_setting:
                print(f"\nüèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: '{best_setting}' (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å {best_start_time:.1f}—Å)")
                
                if best_start_time > 5.0:
                    print("‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å –ø—Ä–æ–ø—É—Å–∫–æ–º –Ω–∞—á–∞–ª–∞ –∞—É–¥–∏–æ!")
                    print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
                    print("   ‚Ä¢ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å model='base' –≤–º–µ—Å—Ç–æ medium/small")
                    print("   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∞—É–¥–∏–æ –≤ –ø–µ—Ä–≤—ã–µ 30 —Å–µ–∫—É–Ω–¥")
                    print("   ‚Ä¢ –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∞—É–¥–∏–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–ª–∏–Ω–Ω—ã—Ö –ø–∞—É–∑ –≤ –Ω–∞—á–∞–ª–µ")
                else:
                    print("‚úÖ –ü—Ä–æ–±–ª–µ–º —Å –ø—Ä–æ–ø—É—Å–∫–æ–º –Ω–∞—á–∞–ª–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
            
            return
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—É–¥–∏–æ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        result = processor.process(
            audio_file, 
            output,
            min_speakers=min_speakers,
            max_speakers=max_speakers,
            min_segment_duration=min_segment,
            alignment_strategy=alignment_strategy,
            time_limit=time_limit
        )
        
        print("\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(result['segments'])}")
        
        if result['has_speaker_diarization']:
            speakers = set(seg['speaker'] for seg in result['segments'])
            unknown_count = sum(1 for seg in result['segments'] if seg['speaker'] == 'Unknown')
            
            print(f"üë• –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(speakers)} ({', '.join(speakers)})")
            print(f"üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–≤–º–µ—â–µ–Ω–∏—è: {result['alignment_strategy']}")
            
            if unknown_count > 0:
                print(f"‚ö†Ô∏è  Unknown —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {unknown_count} ({unknown_count/len(result['segments'])*100:.1f}%)")
            else:
                print("‚úÖ –í—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–≤—è–∑–∞–Ω—ã –∫ —Å–ø–∏–∫–µ—Ä–∞–º!")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            stats = result.get('diarization_stats', {})
            if stats:
                print(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏:")
                print(f"   ‚Ä¢ –°–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {stats.get('segments_before_filter', 'N/A')}")
                print(f"   ‚Ä¢ –°–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {stats.get('segments_after_filter', 'N/A')}")
                print(f"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤: {stats.get('unique_speakers', 'N/A')}")
        
        print(f"üî§ –Ø–∑—ã–∫: {result['language']}")
        print(f"üìù –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(result['transcription'])} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {result['transcription_time']:.1f}—Å")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {result['diarization_time']:.1f}—Å")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 