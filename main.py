#!/usr/bin/env python3
"""
Whisper + PyAnnote Audio Pipeline
–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤
"""

import os
import sys
import warnings
import tempfile
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import click
import torch
import whisper
import librosa
import soundfile as sf
from pyannote.audio import Pipeline
from pyannote.core import Segment
import pandas as pd
from tqdm import tqdm
import time

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings("ignore")


class AudioProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è"""
    
    def __init__(self, whisper_model: str = "base", hf_token: Optional[str] = None, 
                 local_models_dir: Optional[str] = None, device: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            whisper_model: –ú–æ–¥–µ–ª—å Whisper (tiny, base, small, medium, large)
            hf_token: HuggingFace —Ç–æ–∫–µ–Ω –¥–ª—è PyAnnotate
            local_models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (cpu, cuda, mps)
        """
        self.whisper_model_name = whisper_model
        self.hf_token = hf_token
        self.local_models_dir = Path(local_models_dir) if local_models_dir else None
        
        # –í—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if device is not None:
            self.device = device
        else:
            if torch.cuda.is_available():
                self.device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = "mps"  # Apple Silicon
            else:
                self.device = "cpu"
            
        print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
        self._load_models()
    
    def _load_local_config(self) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π"""
        if not self.local_models_dir:
            return None
            
        config_file = self.local_models_dir / "local_config.json"
        if not config_file.exists():
            return None
            
        try:
            with open(config_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return None
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π Whisper –∏ PyAnnotate"""
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Whisper...")
        self.whisper_model = whisper.load_model(
            self.whisper_model_name, 
            device=self.device
        )
        
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏...")
        self.diarization_pipeline = None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        local_config = self._load_local_config()
        
        if local_config and self.local_models_dir:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            try:
                pyannote_model_path = self.local_models_dir / "pyannote" / "speaker-diarization-3.1"
                
                if pyannote_model_path.exists():
                    print(f"üè† –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–∑: {pyannote_model_path}")
                    self.diarization_pipeline = Pipeline.from_pretrained(str(pyannote_model_path))
                    
                    if self.device != "cpu":
                        self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
                    
                    print("‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                    return
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
                print("üîÑ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ HuggingFace...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ HuggingFace
        try:
            if not self.hf_token:
                print("‚ö†Ô∏è  HuggingFace —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")
                print("üí° –î–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏")
                print("üí° –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏: python download_models.py")
                return
                
            self.diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=self.hf_token
            )
            
            if self.device != "cpu":
                self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
                
            print("‚úÖ –ú–æ–¥–µ–ª—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ HuggingFace")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å HuggingFace —Ç–æ–∫–µ–Ω –∏ –¥–æ—Å—Ç—É–ø –∫ pyannote/speaker-diarization-3.1")
            print("üí° –ò–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ: python download_models.py")
            self.diarization_pipeline = None
    
    def _prepare_audio(self, audio_path: str) -> str:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            
        Returns:
            –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
        """
        audio_path = Path(audio_path)
        
        if not audio_path.exists():
            raise FileNotFoundError(f"–ê—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")
        
        # –ï—Å–ª–∏ —É–∂–µ wav —Ñ–∞–π–ª, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if audio_path.suffix.lower() == '.wav':
            return str(audio_path)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ wav
        print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –≤ WAV —Ñ–æ—Ä–º–∞—Ç...")
        audio, sr = librosa.load(str(audio_path), sr=16000)
        
        temp_wav = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
        sf.write(temp_wav.name, audio, sr)
        
        return temp_wav.name
    
    def transcribe(self, audio_path: str) -> Dict:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é Whisper
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        """
        print("üé§ –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é...")
        
        result = self.whisper_model.transcribe(
            audio_path,
            language="ru",  # –ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å auto-detection
            word_timestamps=True
        )
        
        return result
    
    def diarize(self, audio_path: str) -> Optional[Dict]:
        """
        –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é PyAnnotate
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ None –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
        """
        if self.diarization_pipeline is None:
            print("‚ö†Ô∏è  –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ - –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return None
            
        print("üë• –ù–∞—á–∏–Ω–∞–µ–º –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å–ø–∏–∫–µ—Ä–æ–≤...")
        
        try:
            diarization = self.diarization_pipeline(audio_path)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            speakers = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speakers.append({
                    "start": turn.start,
                    "end": turn.end,
                    "speaker": speaker
                })
            
            return {"speakers": speakers}
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return None
    
    def _align_transcription_with_speakers(self, transcription: Dict, diarization: Dict) -> List[Dict]:
        """
        –°–æ–≤–º–µ—â–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–ø–∏–∫–µ—Ä–∞—Ö
        
        Args:
            transcription: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            diarization: –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ —Å–ø–∏–∫–µ—Ä–∞–º–∏
        """
        if not diarization or "speakers" not in diarization:
            # –ï—Å–ª–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
            segments = []
            for segment in transcription.get("segments", []):
                segments.append({
                    "start": segment["start"],
                    "end": segment["end"],
                    "text": segment["text"].strip(),
                    "speaker": "Unknown"
                })
            return segments
        
        # –°–æ–≤–º–µ—â–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é
        aligned_segments = []
        speaker_segments = diarization["speakers"]
        transcription_segments = transcription.get("segments", [])
        
        for t_segment in transcription_segments:
            t_start, t_end = t_segment["start"], t_segment["end"]
            t_mid = (t_start + t_end) / 2
            
            # –ù–∞—Ö–æ–¥–∏–º —Å–ø–∏–∫–µ—Ä–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            best_speaker = "Unknown"
            best_overlap = 0
            
            for s_segment in speaker_segments:
                s_start, s_end = s_segment["start"], s_segment["end"]
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
                overlap_start = max(t_start, s_start)
                overlap_end = min(t_end, s_end)
                overlap = max(0, overlap_end - overlap_start)
                
                if overlap > best_overlap:
                    best_overlap = overlap
                    best_speaker = s_segment["speaker"]
            
            aligned_segments.append({
                "start": t_start,
                "end": t_end,
                "text": t_segment["text"].strip(),
                "speaker": best_speaker
            })
        
        return aligned_segments
    
    def process(self, audio_path: str, output_dir: str = "output") -> Dict:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤—ã–≤–æ–¥–∞
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—É–¥–∏–æ
        prepared_audio = self._prepare_audio(audio_path)
        
        try:
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            start_time = time.time()
            transcription_result = self.transcribe(prepared_audio)
            transcription_time = time.time() - start_time
            
            # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è
            start_time = time.time()
            diarization_result = self.diarize(prepared_audio)
            diarization_time = time.time() - start_time
            
            # –°–æ–≤–º–µ—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            aligned_segments = self._align_transcription_with_speakers(
                transcription_result, 
                diarization_result
            )
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "audio_file": str(Path(audio_path).name),
                "transcription": transcription_result.get("text", ""),
                "segments": aligned_segments,
                "language": transcription_result.get("language", "unknown"),
                "has_speaker_diarization": diarization_result is not None,
                "transcription_time": transcription_time,
                "diarization_time": diarization_time
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self._save_results(result, output_path, Path(audio_path).stem)
            
            return result
            
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –æ–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω
            if prepared_audio != audio_path and os.path.exists(prepared_audio):
                os.unlink(prepared_audio)
    
    def _save_results(self, result: Dict, output_path: Path, base_name: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""
        
        # JSON
        json_path = output_path / f"{base_name}_result.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {json_path}")
        
        # CSV
        if result["segments"]:
            df = pd.DataFrame(result["segments"])
            csv_path = output_path / f"{base_name}_segments.csv"
            df.to_csv(csv_path, index=False, encoding='utf-8')
            print(f"üíæ –°–µ–≥–º–µ–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
        
        # TXT (—á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç)
        txt_path = output_path / f"{base_name}_transcript.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {result['audio_file']}\n")
            f.write(f"–Ø–∑—ã–∫: {result['language']}\n")
            f.write(f"–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è: {'–î–∞' if result['has_speaker_diarization'] else '–ù–µ—Ç'}\n\n")
            
            if result["has_speaker_diarization"]:
                f.write("=== –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø –ü–û –°–ü–ò–ö–ï–†–ê–ú ===\n\n")
                current_speaker = None
                for segment in result["segments"]:
                    if segment["speaker"] != current_speaker:
                        current_speaker = segment["speaker"]
                        f.write(f"\n[{current_speaker}]:\n")
                    
                    start_time = f"{int(segment['start']//60):02d}:{int(segment['start']%60):02d}"
                    f.write(f"{start_time} - {segment['text']}\n")
            else:
                f.write("=== –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø ===\n\n")
                f.write(result["transcription"])
        
        print(f"üíæ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {txt_path}")


@click.command()
@click.argument('audio_file', type=click.Path(exists=True))
@click.option('--model', '-m', default='large', 
              type=click.Choice(['tiny', 'base', 'small', 'medium', 'large']),
              help='–ú–æ–¥–µ–ª—å Whisper –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è')
@click.option('--output', '-o', default='output', 
              help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
@click.option('--hf-token', envvar='HUGGINGFACE_TOKEN', 
              help='HuggingFace —Ç–æ–∫–µ–Ω (–º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π HUGGINGFACE_TOKEN)')
@click.option('--local-models', envvar='LOCAL_MODELS_DIR',
              help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (–º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π LOCAL_MODELS_DIR)')
@click.option('--device', default=None, type=click.Choice(['cpu', 'cuda', 'mps']), help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (cpu, cuda, mps)')
def main(audio_file: str, model: str, output: str, hf_token: Optional[str], local_models: Optional[str], device: Optional[str]):
    """
    –ü–∞–π–ø–ª–∞–π–Ω —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ
    
    AUDIO_FILE: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    
    print("üéµ Whisper + PyAnnote Audio Pipeline")
    print(f"üìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {audio_file}")
    print(f"üß† –ú–æ–¥–µ–ª—å Whisper: {model}")
    print(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output}")
    
    if local_models:
        print(f"üè† –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑: {local_models}")
    elif not hf_token:
        print("‚ö†Ô∏è  HuggingFace —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
        print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ç–æ–∫–µ–Ω: export HUGGINGFACE_TOKEN=your_token")
        print("üí° –ò–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ: python download_models.py")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        processor = AudioProcessor(
            whisper_model=model, 
            hf_token=hf_token,
            local_models_dir=local_models,
            device=device
        )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—É–¥–∏–æ
        result = processor.process(audio_file, output)
        
        print("\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(result['segments'])}")
        
        if result['has_speaker_diarization']:
            speakers = set(seg['speaker'] for seg in result['segments'])
            print(f"üë• –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(speakers)} ({', '.join(speakers)})")
        
        print(f"üî§ –Ø–∑—ã–∫: {result['language']}")
        print(f"üìù –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(result['transcription'])} —Å–∏–º–≤–æ–ª–æ–≤")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 